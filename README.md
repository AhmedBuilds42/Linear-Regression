# Linear-Regression
 Linear Regression with Outliers A practical walkthrough of linear regression using synthetic data with outliers. Demonstrates the mathematical intuition behind the best-fit line and how outliers distort model accuracy. Visualizations included to show error minimization and regression fitting.




Corporateâ€‘speak version: We run a least-squares optimization pipeline to minimize the total â€œerror gapâ€ between our predictions and reality.

Plain English version: For every point, we check how far it is from our line (thatâ€™s called error or residual). We adjust the lineâ€™s slope and position again and again until the overall error is as small as mathematically possible.

The final line is:

ğ‘¦ = ğ‘šğ‘¥ + ğ‘
Where:

m = slope (how steeply your output changes when input changes)

b = intercept (where your line crosses the Y-axis when X=0)

Linear regression = bestâ€‘fit straight line to make predictions.

Why is it useful?

To predict: â€œIf we spend $10k more on marketing, how many more units might we sell?â€

To understand relationships: â€œDoes more study time actually lead to higher scores?â€

To simplify complexity: real-world messy data â†’ one clean predictive rule.
